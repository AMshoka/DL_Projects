{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.core.fromnumeric import size\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import fashion'mnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot 2*2 grid from four random fashion'mnist dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list=random.sample(range(0, 60000), 4)\n",
    "for i in range(4):\n",
    "    plt.subplot(220 + 1 + (i))\n",
    "    plt.imshow(x_train[rand_list[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1 Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2 ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0.0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation derivative activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 Sigmoid Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid_D(x):\n",
    "    return np.multiply(x,(1-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 ReLU Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_D(x):\n",
    "    return 1 * (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 Softmax Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax_D(x):\n",
    "    exps = np.exp(x - x.max())\n",
    "    return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Multi_cross_entropy(y, ypred):\n",
    "    return -sum([y[i]*np.log(ypred[i]) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation SGD and Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(deri_b1,deri_b2,deri_b3,deri_w1,deri_w2,deri_w3,lr,b1,b2,b3,w1,w2,w3):\n",
    "    self.b3 -= self.lr*self.deri_b3\n",
    "    self.w3 -= self.lr*self.deri_w3\n",
    "    self.b2 -= self.lr*self.deri_b2\n",
    "    self.w2 -= self.lr*self.deri_w2\n",
    "    self.b1 -= self.lr*self.deri_b1\n",
    "    self.w1 -= self.lr*self.deri_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Momentum(deri_b1,deri_b2,deri_b3,deri_w1,deri_w2,deri_w3,momentum_b1,momentum_b2,momentum_b3,\n",
    "            momentum_w1,momentum_w2,momentum_w3,beta1,beta2,b1,b2,b3,w1,w2,w3,lr):\n",
    "    \n",
    "    momentum_b3 = beta1*momentum_b3 + (1-beta1)*deri_b3\n",
    "    momentum_w3 = beta1*momentum_w3 + (1-beta1)*deri_w3\n",
    "    momentum_b2 = beta1*momentum_b2 + (1-beta1)*deri_b2\n",
    "    momentum_w2 = beta1*momentum_w2 + (1-beta1)*deri_w2\n",
    "    momentum_b1 = beta1*momentum_b1 + (1-beta1)*deri_b1\n",
    "    momentum_w1 = beta1*momentum_w1 + (1-beta1)*deri_w1\n",
    "    ################update###########\n",
    "    b3 -= momentum_b3\n",
    "    w3 -= momentum_w3\n",
    "    b2 -= momentum_b2\n",
    "    w2 -= momentum_w2\n",
    "    b1 -= momentum_b1\n",
    "    w1 -= momentum_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Neural Network (784,128,64,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    ################intialize parameter###########\n",
    "    samples = 60000\n",
    "    batch_size = 1\n",
    "    epochs = 3\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    constant = 1e-8\n",
    "    lr = 1e-3\n",
    "    Lambda = 0\n",
    "    \n",
    "    def __init__(self, sizes, optimizer, hiddenActivation, sechiddenActivation, outputActivation): \n",
    "        self.sizes = sizes\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.hiddenActivation = hiddenActivation\n",
    "        self.sechiddenActivation=sechiddenActivation\n",
    "        self.outputActivation = outputActivation\n",
    "\n",
    "        self.x = np.arange(1,((self.samples/self.batch_size)*self.epochs)+1)\n",
    "        self.y = np.empty(int(((self.samples/self.batch_size)*self.epochs)))\n",
    "        self.secondLayerNeurons = np.empty(sizes[1])\n",
    "        self.thirdLayerNeurons = np.empty(sizes[2])\n",
    "        self.outputNeurons = np.empty(sizes[3])\n",
    "        \n",
    "        self.w1 = np.random.rand(sizes[1], sizes[0]) * 2 - 1\n",
    "        self.w2 = np.random.rand(sizes[2], sizes[1]) * 2 - 1\n",
    "        self.w3 = np.random.rand(sizes[3], sizes[2]) * 2 - 1\n",
    "        self.b1 = np.zeros([sizes[1]])\n",
    "        self.b2 = np.zeros([sizes[2]])\n",
    "        self.b3 = np.zeros([sizes[3]])\n",
    "        \n",
    "        self.deri_w1 = np.zeros([sizes[1], sizes[0]])\n",
    "        self.deri_w2 = np.zeros([sizes[2], sizes[1]])\n",
    "        self.deri_w3 = np.zeros([sizes[3], sizes[2]])\n",
    "        \n",
    "        self.deri_b1 = np.zeros([sizes[1]])\n",
    "        self.deri_b2 = np.zeros([sizes[2]])\n",
    "        self.deri_b3 = np.zeros([sizes[3]])\n",
    "        \n",
    "        self.momentum_w1 = np.zeros([sizes[1], sizes[0]])\n",
    "        self.momentum_w2 = np.zeros([sizes[2], sizes[1]])\n",
    "        self.momentum_w3 = np.zeros([sizes[3], sizes[2]])\n",
    "        self.momentum_b1 = np.zeros([sizes[1]])\n",
    "        self.momentum_b2 = np.zeros([sizes[2]])\n",
    "        self.momentum_b3 = np.zeros([sizes[3]])\n",
    "\n",
    "        self.RMS_prob_w1 = np.zeros([sizes[1], sizes[0]])\n",
    "        self.RMS_prob_w2 = np.zeros([sizes[2], sizes[1]])\n",
    "        self.RMS_prob_w3 = np.zeros([sizes[3], sizes[2]])\n",
    "        \n",
    "        self.RMS_prob_b1 = np.zeros([sizes[1]])\n",
    "        self.RMS_prob_b2 = np.zeros([sizes[2]])\n",
    "        self.RMS_prob_b3 = np.zeros([sizes[3]])\n",
    "        \n",
    "        self.momentum_w1_revision = np.empty([sizes[1], sizes[0]])\n",
    "        self.momentum_w2_revision = np.empty([sizes[2], sizes[1]])\n",
    "        self.momentum_w3_revision = np.empty([sizes[3], sizes[2]])\n",
    "        self.momentum_b1_revision = np.empty(sizes[1])\n",
    "        self.momentum_b2_revision = np.empty(sizes[2])\n",
    "        self.momentum_b3_revision = np.empty(sizes[3])\n",
    "\n",
    "        self.RMS_prob_w1_revision = np.empty([sizes[1], sizes[0]])\n",
    "        self.RMS_prob_w2_revision = np.empty([sizes[2], sizes[1]])\n",
    "        self.RMS_prob_w3_revision = np.empty([sizes[3], sizes[2]])\n",
    "        self.RMS_prob_b1_revision = np.empty(sizes[1])\n",
    "        self.RMS_prob_b2_revision = np.empty(sizes[2])\n",
    "        self.RMS_prob_b3_revision = np.empty(sizes[3])\n",
    "        \n",
    "        self.hiddenLayerErrors = np.empty(sizes[1])\n",
    "        self.sechiddenLayerErrors = np.empty(sizes[2])\n",
    "        self.outputLayerErrors = np.empty(sizes[3])\n",
    "        self.cceoutputLayerErrors = np.empty(sizes[3])\n",
    "    ################define activation and int's derivative###########\n",
    "    \n",
    "    def Multi_cross_entropy(self, y, ypred):\n",
    "        return -sum([y[i]*np.log(ypred[i]) for i in range(len(y))])    \n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def sigmoidDerivative(self, x):\n",
    "        return np.multiply(x,(1-x))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0.0, x)\n",
    "    \n",
    "    def reluDerivative(self, x):\n",
    "        return 1 * (x > 0)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - np.max(x))\n",
    "        return exps / np.sum(exps)\n",
    "    ################forward path###########\n",
    "    def forwardProp(self, inputs):\n",
    "        if self.hiddenActivation == 'sigmoid':\n",
    "            self.secondLayerNeurons = self.sigmoid(self.w1 @ inputs + self.b1)\n",
    "        elif self.hiddenActivation == 'relu':\n",
    "            self.secondLayerNeurons = self.relu(self.w1 @ inputs + self.b1)\n",
    "\n",
    "        if self.sechiddenActivation == 'sigmoid':\n",
    "            self.thirdLayerNeurons = self.sigmoid(self.w2 @ self.secondLayerNeurons + self.b2)\n",
    "        elif self.sechiddenActivation == 'softmax':\n",
    "            self.thirdLayerNeurons = self.softmax(self.w2 @ self.secondLayerNeurons + self.b2)\n",
    "            \n",
    "        if self.outputActivation == 'sigmoid':\n",
    "            self.outputNeurons = self.sigmoid(self.w3 @ self.thirdLayerNeurons + self.b3)\n",
    "        elif self.outputActivation == 'softmax':\n",
    "            self.outputNeurons = self.softmax(self.w3 @ self.thirdLayerNeurons + self.b3)\n",
    "            len(self.outputNeurons.shape)\n",
    "    ################backward path###########        \n",
    "    def backProp(self, inputs, revision_output):\n",
    "        self.cceoutputLayerErrors=self.Multi_cross_entropy(revision_output,self.outputNeurons)\n",
    "        self.outputLayerErrors = np.subtract(self.outputNeurons, revision_output)\n",
    "        if self.sechiddenActivation == 'sigmoid':\n",
    "            self.sechiddenLayerErrors = np.multiply(np.dot(self.w3.T, self.outputLayerErrors), self.sigmoidDerivative(self.thirdLayerNeurons))\n",
    "        elif self.sechiddenActivation == 'relu':\n",
    "            self.sechiddenLayerErrors = np.multiply(np.dot(self.w3.T, self.outputLayerErrors), self.reluDerivative(self.thirdLayerNeurons))\n",
    "        if self.hiddenActivation == 'sigmoid':\n",
    "            self.hiddenLayerErrors = np.multiply(np.dot(self.w2.T, self.sechiddenLayerErrors), self.sigmoidDerivative(self.secondLayerNeurons))\n",
    "        elif self.hiddenActivation == 'relu':\n",
    "            self.hiddenLayerErrors = np.multiply(np.dot(self.w2.T, self.sechiddenLayerErrors), self.reluDerivative(self.secondLayerNeurons))\n",
    "    \n",
    "        self.deri_b3 += self.outputLayerErrors\n",
    "        self.deri_w3 += np.dot(self.outputLayerErrors.reshape(self.sizes[3],1), self.thirdLayerNeurons.reshape(1,self.sizes[2]))\n",
    "        self.deri_b2 += self.sechiddenLayerErrors\n",
    "        self.deri_w2 += np.dot(self.sechiddenLayerErrors.reshape(self.sizes[2],1), self.secondLayerNeurons.reshape(1,self.sizes[1]))      \n",
    "        self.deri_b1 += self.hiddenLayerErrors\n",
    "        self.deri_w1 += np.dot(self.hiddenLayerErrors.reshape(self.sizes[1],1), inputs.reshape(1,self.sizes[0]))\n",
    "     \n",
    "    ################update weight###########\n",
    "    def Update_weight(self):\n",
    "            \n",
    "        if self.optimizer == 'momentum':\n",
    "            self.momentum_b3 = self.beta1*self.momentum_b3 + self.lr*self.deri_b3\n",
    "            self.momentum_w3 = self.beta1*self.momentum_w3 + self.lr*self.deri_w3\n",
    "            self.momentum_b2 = self.beta1*self.momentum_b2 + self.lr*self.deri_b2\n",
    "            self.momentum_w2 = self.beta1*self.momentum_w2 + self.lr*self.deri_w2\n",
    "            self.momentum_b1 = self.beta1*self.momentum_b1 + self.lr*self.deri_b1\n",
    "            self.momentum_w1 = self.beta1*self.momentum_w1 + self.lr*self.deri_w1\n",
    "\n",
    "            self.b3 -= self.momentum_b3\n",
    "            self.w3 -= self.momentum_w3\n",
    "            self.b2 -= self.momentum_b2\n",
    "            self.w2 -= self.momentum_w2\n",
    "            self.b1 -= self.momentum_b1\n",
    "            self.w1 -= self.momentum_w1\n",
    "            \n",
    "        elif self.optimizer == 'SGD':\n",
    "            self.b3 -= self.lr*self.deri_b3\n",
    "            self.w3 -= self.lr*self.deri_w3\n",
    "            self.b2 -= self.lr*self.deri_b2\n",
    "            self.w2 -= self.lr*self.deri_w2\n",
    "            self.b1 -= self.lr*self.deri_b1\n",
    "            self.w1 -= self.lr*self.deri_w1\n",
    "        elif self.optimizer == 'adam':\n",
    "            self.momentum_b3 = self.beta1*self.momentum_b3 + (1-self.beta1)*self.deri_b3\n",
    "            self.momentum_w3 = self.beta1*self.momentum_w3 + (1-self.beta1)*self.deri_w3\n",
    "            self.momentum_b2 = self.beta1*self.momentum_b2 + (1-self.beta1)*self.deri_b2\n",
    "            self.momentum_w2 = self.beta1*self.momentum_w2 + (1-self.beta1)*self.deri_w2\n",
    "            self.momentum_b1 = self.beta1*self.momentum_b1 + (1-self.beta1)*self.deri_b1\n",
    "            self.momentum_w1 = self.beta1*self.momentum_w1 + (1-self.beta1)*self.deri_w1\n",
    "\n",
    "            self.RMS_prob_b3 = self.beta2*self.RMS_prob_b3 + (1-self.beta2)*np.square(self.deri_b3)\n",
    "            self.RMS_prob_w3 = self.beta2*self.RMS_prob_w3 + (1-self.beta2)*np.square(self.deri_w3)\n",
    "            self.RMS_prob_b2 = self.beta2*self.RMS_prob_b2 + (1-self.beta2)*np.square(self.deri_b2)\n",
    "            self.RMS_prob_w2 = self.beta2*self.RMS_prob_w2 + (1-self.beta2)*np.square(self.deri_w2)\n",
    "            self.RMS_prob_b1 = self.beta2*self.RMS_prob_b1 + (1-self.beta2)*np.square(self.deri_b1)\n",
    "            self.RMS_prob_w1 = self.beta2*self.RMS_prob_w1 + (1-self.beta2)*np.square(self.deri_w1)\n",
    "\n",
    "\n",
    "            self.momentum_b3_revision = self.momentum_b3/(1-np.power(self.beta1, self.batch_size))\n",
    "            self.momentum_w3_revision = self.momentum_w3/(1-np.power(self.beta1, self.batch_size))\n",
    "            self.momentum_b2_revision = self.momentum_b2/(1-np.power(self.beta1, self.batch_size))\n",
    "            self.momentum_w2_revision = self.momentum_w2/(1-np.power(self.beta1, self.batch_size))\n",
    "            self.momentum_b1_revision = self.momentum_b1/(1-np.power(self.beta1, self.batch_size))\n",
    "            self.momentum_w1_revision = self.momentum_w1/(1-np.power(self.beta1, self.batch_size))\n",
    "\n",
    "            self.RMS_prob_b3_revision = self.RMS_prob_b3/(1-np.power(self.beta2, self.batch_size))\n",
    "            self.RMS_prob_w3_revision = self.RMS_prob_w3/(1-np.power(self.beta2, self.batch_size))\n",
    "            self.RMS_prob_b2_revision = self.RMS_prob_b2/(1-np.power(self.beta2, self.batch_size))\n",
    "            self.RMS_prob_w2_revision = self.RMS_prob_w2/(1-np.power(self.beta2, self.batch_size))\n",
    "            self.RMS_prob_b1_revision = self.RMS_prob_b1/(1-np.power(self.beta2, self.batch_size))\n",
    "            self.RMS_prob_w1_revision = self.RMS_prob_w1/(1-np.power(self.beta2, self.batch_size))\n",
    "\n",
    "            self.b3 -= self.lr * (self.momentum_b3_revision/(np.sqrt(self.RMS_prob_b3_revision)+self.constant))\n",
    "            self.w3 -= self.lr * (self.momentum_w3_revision/(np.sqrt(self.RMS_prob_w3_revision)+self.constant))\n",
    "            self.b2 -= self.lr * (self.momentum_b2_revision/(np.sqrt(self.RMS_prob_b2_revision)+self.constant))\n",
    "            self.w2 -= self.lr * (self.momentum_w2_revision/(np.sqrt(self.RMS_prob_w2_revision)+self.constant))\n",
    "            self.b1 -= self.lr * (self.momentum_b1_revision/(np.sqrt(self.RMS_prob_b1_revision)+self.constant))\n",
    "            self.w1 -= self.lr * (self.momentum_w1_revision/(np.sqrt(self.RMS_prob_w1_revision)+self.constant))\n",
    "\n",
    "        \n",
    "\n",
    "        self.deri_w1 = np.zeros([self.sizes[1], self.sizes[0]])\n",
    "        self.deri_w2 = np.zeros([self.sizes[2], self.sizes[1]])\n",
    "        self.deri_w3 = np.zeros([self.sizes[3], self.sizes[2]])\n",
    "        self.deri_b1 = np.zeros(self.sizes[1])\n",
    "        self.deri_b2 = np.zeros(self.sizes[2])\n",
    "        self.deri_b3 = np.zeros(self.sizes[3])\n",
    "        \n",
    "    ################training phase###########    \n",
    "    def train(self, trainImages, trainLabels):\n",
    "        size = str(self.batch_size)\n",
    "        accuracy = 0\n",
    "        err_sum = 0.0\n",
    "        avg_err = 0.0\n",
    "        revision = 0\n",
    "\n",
    "        batch_start_time = time.time()\n",
    "        for m in range (self.batch_size):\n",
    "            revision_output = np.zeros([self.sizes[3]])\n",
    "            revision_output[trainLabels[m]] = 1.0\n",
    "            self.forwardProp(trainImages[m].flatten())\n",
    "            self.backProp(trainImages[m].flatten(), revision_output)\n",
    "\n",
    "            if np.argmax(self.outputNeurons) == int(trainLabels[m]):\n",
    "                revision+=1\n",
    "\n",
    "            error = np.amax(np.absolute(self.cceoutputLayerErrors))\n",
    "            print(\"Error:\")\n",
    "            print(error)\n",
    "            err_sum += error\n",
    "            avg_err = err_sum / (m+1)\n",
    "            acc = str(int((revision/(m+1))*100)) + '%'\n",
    "            print(\"Accuracy:\")\n",
    "            print(acc)\n",
    "            \n",
    "            \n",
    "        self.Update_weight()\n",
    "        return avg_err\n",
    "    ################prediction###########\n",
    "    def predict(self, testImage):\n",
    "        self.forwardProp(testImage)\n",
    "        return np.argmax(self.outputNeurons), self.outputNeurons\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase based on sigmoid activation and SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images = train_images/255\n",
    "inst_mlp = MLP([784, 128,64, 10], 'SGD', 'sigmoid','sigmoid', 'softmax')\n",
    "train_err=[]\n",
    "test_err=[]\n",
    "for i in range (inst_mlp.epochs):\n",
    "    print('----------------------'+'Epoch'+str(i+1)+'----------------------')\n",
    "    for j in range(int(inst_mlp.samples/inst_mlp.batch_size)):\n",
    "        print(\"Epoch\", str(i+1) + \"/\" + str(inst_mlp.epochs) + \":\" )\n",
    "        inst_mlp.y[j+i*(int(inst_mlp.samples/inst_mlp.batch_size))] = inst_mlp.train(train_images[int(j * inst_mlp.batch_size):int(j * inst_mlp.batch_size) + inst_mlp.batch_size], train_labels[int(j * inst_mlp.batch_size):int(j * inst_mlp.batch_size) + inst_mlp.batch_size])\n",
    "    size1 = test_images.shape[0]\n",
    "    temp_err=0\n",
    "    for i in range (size1):\n",
    "        prediction,yp = inst_mlp.predict(test_images[i].flatten())\n",
    "        yt = np.zeros([10])\n",
    "        yt[test_labels[i]] = 1.0\n",
    "        cost=Multi_cross_entropy(yt,yp)\n",
    "        temp_err=temp_err+np.amax(np.absolute(cost))\n",
    "        \n",
    "    test_err.append(temp_err)\n",
    "test_err=np.array(test_err)/10000       \n",
    "for i in range(inst_mlp.epochs):\n",
    "    train_err.append((np.sum(inst_mlp.y[60000*i:60000*(i+1)-1]))/60000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfalse = 0\n",
    "size1 = train_images.shape[0]\n",
    "for i in range (size1):\n",
    "    prediction,t = inst_mlp.predict(train_images[i].flatten())\n",
    "    ytrue = int(train_labels[i])\n",
    "    if ytrue != prediction:\n",
    "        yfalse += 1  \n",
    "\n",
    "print('Train_Accuracy:')\n",
    "print(((size1-yfalse)/size1)*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfalse = 0\n",
    "size1 = test_images.shape[0]\n",
    "for i in range (size1):\n",
    "    prediction,t = inst_mlp.predict(test_images[i].flatten())\n",
    "    ytrue = int(test_labels[i])\n",
    "    if ytrue != prediction:\n",
    "        yfalse += 1  \n",
    "\n",
    "print('Test_Accuracy:')\n",
    "print(((size1-yfalse)/size1)*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot cost function for testing phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,inst_mlp.epochs+1), np.array(test_err))\n",
    "plt.ylabel('Test Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot cost function for training phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,inst_mlp.epochs+1), np.array(train_err))\n",
    "plt.ylabel('Train Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase based on ReLU activation and SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images = train_images/255\n",
    "inst_mlp = MLP([784, 128,64, 10], 'SGD', 'relu','relu', 'softmax')\n",
    "train_err=[]\n",
    "test_err=[]\n",
    "for i in range (inst_mlp.epochs):\n",
    "    print('----------------------'+'Epoch'+str(i+1)+'----------------------')\n",
    "    for j in range(int(inst_mlp.samples/inst_mlp.batch_size)):\n",
    "        print(\"Epoch\", str(i+1) + \"/\" + str(inst_mlp.epochs) + \":\" )\n",
    "        inst_mlp.y[j+i*(int(inst_mlp.samples/inst_mlp.batch_size))] = inst_mlp.train(train_images[int(j * inst_mlp.batch_size):int(j * inst_mlp.batch_size) + inst_mlp.batch_size], train_labels[int(j * inst_mlp.batch_size):int(j * inst_mlp.batch_size) + inst_mlp.batch_size])\n",
    "    size1 = test_images.shape[0]\n",
    "    temp_err=0\n",
    "    for i in range (size1):\n",
    "        prediction,yp = inst_mlp.predict(test_images[i].flatten())\n",
    "        yt = np.zeros([10])\n",
    "        yt[test_labels[i]] = 1.0\n",
    "        cost=Multi_cross_entropy(yt,yp)\n",
    "        temp_err=temp_err+np.amax(np.absolute(cost))\n",
    "        \n",
    "    test_err.append(temp_err)\n",
    "test_err=np.array(test_err)/10000       \n",
    "for i in range(inst_mlp.epochs):\n",
    "    train_err.append((np.sum(inst_mlp.y[60000*i:60000*(i+1)-1]))/60000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfalse = 0\n",
    "size1 = train_images.shape[0]\n",
    "for i in range (size1):\n",
    "    prediction,t = inst_mlp.predict(train_images[i].flatten())\n",
    "    ytrue = int(train_labels[i])\n",
    "    if ytrue != prediction:\n",
    "        yfalse += 1  \n",
    "\n",
    "print('Train_Accuracy:')\n",
    "print(((size1-yfalse)/size1)*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfalse = 0\n",
    "size1 = test_images.shape[0]\n",
    "for i in range (size1):\n",
    "    prediction,t = inst_mlp.predict(test_images[i].flatten())\n",
    "    ytrue = int(test_labels[i])\n",
    "    if ytrue != prediction:\n",
    "        yfalse += 1  \n",
    "\n",
    "print('Test_Accuracy:')\n",
    "print(((size1-yfalse)/size1)*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot cost function for testing phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,inst_mlp.epochs+1), np.array(test_err))\n",
    "plt.ylabel('Test Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot cost function for training phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,inst_mlp.epochs+1), np.array(train_err))\n",
    "plt.ylabel('Train Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase based on sigmoid activation and momentum optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images = train_images/255\n",
    "inst_mlp = MLP([784, 128,64, 10], 'momentum', 'sigmoid','sigmoid', 'softmax')\n",
    "train_err=[]\n",
    "test_err=[]\n",
    "for i in range (inst_mlp.epochs):\n",
    "    print('----------------------'+'Epoch'+str(i+1)+'----------------------')\n",
    "    for j in range(int(inst_mlp.samples/inst_mlp.batch_size)):\n",
    "        print(\"Epoch\", str(i+1) + \"/\" + str(inst_mlp.epochs) + \":\" )\n",
    "        inst_mlp.y[j+i*(int(inst_mlp.samples/inst_mlp.batch_size))] = inst_mlp.train(train_images[int(j * inst_mlp.batch_size):int(j * inst_mlp.batch_size) + inst_mlp.batch_size], train_labels[int(j * inst_mlp.batch_size):int(j * inst_mlp.batch_size) + inst_mlp.batch_size])\n",
    "    size1 = test_images.shape[0]\n",
    "    temp_err=0\n",
    "    for i in range (size1):\n",
    "        prediction,yp = inst_mlp.predict(test_images[i].flatten())\n",
    "        yt = np.zeros([10])\n",
    "        yt[test_labels[i]] = 1.0\n",
    "        cost=Multi_cross_entropy(yt,yp)\n",
    "        temp_err=temp_err+np.amax(np.absolute(cost))\n",
    "        \n",
    "    test_err.append(temp_err)\n",
    "test_err=np.array(test_err)/10000       \n",
    "for i in range(inst_mlp.epochs):\n",
    "    train_err.append((np.sum(inst_mlp.y[60000*i:60000*(i+1)-1]))/60000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfalse = 0\n",
    "size1 = train_images.shape[0]\n",
    "for i in range (size1):\n",
    "    prediction,t = inst_mlp.predict(train_images[i].flatten())\n",
    "    ytrue = int(train_labels[i])\n",
    "    if ytrue != prediction:\n",
    "        yfalse += 1  \n",
    "\n",
    "print('Train_Accuracy:')\n",
    "print(((size1-yfalse)/size1)*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfalse = 0\n",
    "size1 = test_images.shape[0]\n",
    "for i in range (size1):\n",
    "    prediction,t = inst_mlp.predict(test_images[i].flatten())\n",
    "    ytrue = int(test_labels[i])\n",
    "    if ytrue != prediction:\n",
    "        yfalse += 1  \n",
    "\n",
    "print('Test_Accuracy:')\n",
    "print(((size1-yfalse)/size1)*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot cost function for testing phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,inst_mlp.epochs+1), np.array(test_err))\n",
    "plt.ylabel('Test Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot cost function for traininig phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,inst_mlp.epochs+1), np.array(train_err))\n",
    "plt.ylabel('Train Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Neural Network (784,128,64,64,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    ################intialize parameter###########\n",
    "    samples = 60000\n",
    "    batch_size = 1\n",
    "    epochs = 3\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    constant = 1e-8\n",
    "    lr = 1e-3\n",
    "    Lambda = 0\n",
    "    \n",
    "    def __init__(self, sizes, optimizer, hiddenActivation, sechiddenActivation,thihiddenActivation, outputActivation): \n",
    "        self.sizes = sizes\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.hiddenActivation = hiddenActivation\n",
    "        self.sechiddenActivation=sechiddenActivation\n",
    "        self.thihiddenActivation=thihiddenActivation\n",
    "        self.outputActivation = outputActivation\n",
    "\n",
    "        self.x = np.arange(1,((self.samples/self.batch_size)*self.epochs)+1)\n",
    "        self.y = np.empty(int(((self.samples/self.batch_size)*self.epochs)))\n",
    "        self.secondLayerNeurons = np.empty(sizes[1])\n",
    "        self.thirdLayerNeurons = np.empty(sizes[2])\n",
    "        self.fourLayerNeurons = np.empty(sizes[3])\n",
    "        self.outputNeurons = np.empty(sizes[4])\n",
    "        \n",
    "        self.w1 = np.random.rand(sizes[1], sizes[0]) * 2 - 1\n",
    "        self.w2 = np.random.rand(sizes[2], sizes[1]) * 2 - 1\n",
    "        self.w3 = np.random.rand(sizes[3], sizes[2]) * 2 - 1\n",
    "        self.w4 = np.random.rand(sizes[4], sizes[3]) * 2 - 1\n",
    "        self.b1 = np.zeros([sizes[1]])\n",
    "        self.b2 = np.zeros([sizes[2]])\n",
    "        self.b3 = np.zeros([sizes[3]])\n",
    "        self.b4 = np.zeros([sizes[4]])\n",
    "        \n",
    "        self.deri_w1 = np.zeros([sizes[1], sizes[0]])\n",
    "        self.deri_w2 = np.zeros([sizes[2], sizes[1]])\n",
    "        self.deri_w3 = np.zeros([sizes[3], sizes[2]])\n",
    "        self.deri_w4 = np.zeros([sizes[4], sizes[3]])\n",
    "        \n",
    "        self.deri_b1 = np.zeros([sizes[1]])\n",
    "        self.deri_b2 = np.zeros([sizes[2]])\n",
    "        self.deri_b3 = np.zeros([sizes[3]])\n",
    "        self.deri_b4 = np.zeros([sizes[4]])\n",
    "        \n",
    "        self.momentum_w1 = np.zeros([sizes[1], sizes[0]])\n",
    "        self.momentum_w2 = np.zeros([sizes[2], sizes[1]])\n",
    "        self.momentum_w3 = np.zeros([sizes[3], sizes[2]])\n",
    "        self.momentum_w4 = np.zeros([sizes[4], sizes[3]])\n",
    "        self.momentum_b1 = np.zeros([sizes[1]])\n",
    "        self.momentum_b2 = np.zeros([sizes[2]])\n",
    "        self.momentum_b3 = np.zeros([sizes[3]])\n",
    "        self.momentum_b4 = np.zeros([sizes[4]])\n",
    "\n",
    "        self.RMS_prob_w1 = np.zeros([sizes[1], sizes[0]])\n",
    "        self.RMS_prob_w2 = np.zeros([sizes[2], sizes[1]])\n",
    "        self.RMS_prob_w3 = np.zeros([sizes[3], sizes[2]])\n",
    "        self.RMS_prob_w4 = np.zeros([sizes[4], sizes[3]])\n",
    "        \n",
    "        self.RMS_prob_b1 = np.zeros([sizes[1]])\n",
    "        self.RMS_prob_b2 = np.zeros([sizes[2]])\n",
    "        self.RMS_prob_b3 = np.zeros([sizes[3]])\n",
    "        self.RMS_prob_b4 = np.zeros([sizes[4]])\n",
    "        \n",
    "        self.momentum_w1_revision = np.empty([sizes[1], sizes[0]])\n",
    "        self.momentum_w2_revision = np.empty([sizes[2], sizes[1]])\n",
    "        self.momentum_w3_revision = np.empty([sizes[3], sizes[2]])\n",
    "        self.momentum_w4_revision = np.empty([sizes[4], sizes[3]])\n",
    "        self.momentum_b1_revision = np.empty(sizes[1])\n",
    "        self.momentum_b2_revision = np.empty(sizes[2])\n",
    "        self.momentum_b3_revision = np.empty(sizes[3])\n",
    "        self.momentum_b4_revision = np.empty(sizes[4])\n",
    "\n",
    "        self.RMS_prob_w1_revision = np.empty([sizes[1], sizes[0]])\n",
    "        self.RMS_prob_w2_revision = np.empty([sizes[2], sizes[1]])\n",
    "        self.RMS_prob_w3_revision = np.empty([sizes[3], sizes[2]])\n",
    "        self.RMS_prob_w4_revision = np.empty([sizes[4], sizes[3]])\n",
    "        self.RMS_prob_b1_revision = np.empty(sizes[1])\n",
    "        self.RMS_prob_b2_revision = np.empty(sizes[2])\n",
    "        self.RMS_prob_b3_revision = np.empty(sizes[3])\n",
    "        self.RMS_prob_b4_revision = np.empty(sizes[4])\n",
    "        \n",
    "        self.hiddenLayerErrors = np.empty(sizes[1])\n",
    "        self.sechiddenLayerErrors = np.empty(sizes[2])\n",
    "        self.thirhiddenLayerErrors = np.empty(sizes[3])\n",
    "        self.outputLayerErrors = np.empty(sizes[4])\n",
    "        self.cceoutputLayerErrors = np.empty(sizes[4])\n",
    "    ################define activation and int's derivative###########\n",
    "    \n",
    "    def Multi_cross_entropy(self, y, ypred):\n",
    "        return -sum([y[i]*np.log(ypred[i]) for i in range(len(y))])    \n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def sigmoidDerivative(self, x):\n",
    "        return np.multiply(x,(1-x))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0.0, x)\n",
    "    \n",
    "    def reluDerivative(self, x):\n",
    "        return 1 * (x > 0)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - np.max(x))\n",
    "        return exps / np.sum(exps)\n",
    "    ################forward path###########\n",
    "    def forwardProp(self, inputs):\n",
    "        if self.hiddenActivation == 'sigmoid':\n",
    "            self.secondLayerNeurons = self.sigmoid(self.w1 @ inputs + self.b1)\n",
    "        elif self.hiddenActivation == 'relu':\n",
    "            self.secondLayerNeurons = self.relu(self.w1 @ inputs + self.b1)\n",
    "\n",
    "        if self.sechiddenActivation == 'sigmoid':\n",
    "            self.thirdLayerNeurons = self.sigmoid(self.w2 @ self.secondLayerNeurons + self.b2)\n",
    "        elif self.sechiddenActivation == 'softmax':\n",
    "            self.thirdLayerNeurons = self.softmax(self.w2 @ self.secondLayerNeurons + self.b2)\n",
    "            \n",
    "        if self.thihiddenActivation == 'sigmoid':\n",
    "            self.fourLayerNeurons = self.sigmoid(self.w3 @ self.thirdLayerNeurons + self.b3)\n",
    "        elif self.thihiddenActivation == 'softmax':\n",
    "            self.fourLayerNeurons = self.softmax(self.w3 @ self.thirdLayerNeurons + self.b3)\n",
    "            \n",
    "        if self.outputActivation == 'sigmoid':\n",
    "            self.outputNeurons = self.sigmoid(self.w4 @ self.fourLayerNeurons+ self.b4)\n",
    "        elif self.outputActivation == 'softmax':\n",
    "            self.outputNeurons = self.softmax(self.w4 @ self.fourLayerNeurons + self.b4)\n",
    "        \n",
    "    ################backward path###########        \n",
    "    def backProp(self, inputs, revision_output):\n",
    "        self.cceoutputLayerErrors=self.Multi_cross_entropy(revision_output,self.outputNeurons)\n",
    "        self.outputLayerErrors = np.subtract(self.outputNeurons, revision_output)\n",
    "        \n",
    "        if self.thihiddenActivation == 'sigmoid':\n",
    "            self.thirhiddenLayerErrors = np.multiply(np.dot(self.w4.T, self.outputLayerErrors), self.sigmoidDerivative(self.fourLayerNeurons))\n",
    "        elif self.thihiddenActivation == 'relu':\n",
    "            self.thirhiddenLayerErrors = np.multiply(np.dot(self.w4.T, self.outputLayerErrors), self.reluDerivative(self.fourLayerNeurons))\n",
    "            \n",
    "        if self.sechiddenActivation == 'sigmoid':\n",
    "            self.sechiddenLayerErrors = np.multiply(np.dot(self.w3.T, self.thirhiddenLayerErrors), self.sigmoidDerivative(self.thirdLayerNeurons))\n",
    "        elif self.sechiddenActivation == 'relu':\n",
    "            self.sechiddenLayerErrors = np.multiply(np.dot(self.w3.T, self.thirhiddenLayerErrors), self.reluDerivative(self.thirdLayerNeurons))\n",
    "            \n",
    "        if self.hiddenActivation == 'sigmoid':\n",
    "            self.hiddenLayerErrors = np.multiply(np.dot(self.w2.T, self.sechiddenLayerErrors), self.sigmoidDerivative(self.secondLayerNeurons))\n",
    "        elif self.hiddenActivation == 'relu':\n",
    "            self.hiddenLayerErrors = np.multiply(np.dot(self.w2.T, self.sechiddenLayerErrors), self.reluDerivative(self.secondLayerNeurons))\n",
    "    \n",
    "        self.deri_b4 += self.outputLayerErrors\n",
    "        self.deri_w4 += np.dot(self.outputLayerErrors.reshape(self.sizes[4],1), self.fourLayerNeurons.reshape(1,self.sizes[3]))\n",
    "        self.deri_b3 += self.thirhiddenLayerErrors\n",
    "        self.deri_w3 += np.dot(self.thirhiddenLayerErrors.reshape(self.sizes[3],1), self.thirdLayerNeurons.reshape(1,self.sizes[2]))\n",
    "        self.deri_b2 += self.sechiddenLayerErrors\n",
    "        self.deri_w2 += np.dot(self.sechiddenLayerErrors.reshape(self.sizes[2],1), self.secondLayerNeurons.reshape(1,self.sizes[1]))      \n",
    "        self.deri_b1 += self.hiddenLayerErrors\n",
    "        self.deri_w1 += np.dot(self.hiddenLayerErrors.reshape(self.sizes[1],1), inputs.reshape(1,self.sizes[0]))\n",
    "     \n",
    "    ################update weight###########\n",
    "    def Update_weight(self):\n",
    "            \n",
    "        if self.optimizer == 'momentum':\n",
    "            self.momentum_b4 = self.beta1*self.momentum_b4 + self.lr*self.deri_b4\n",
    "            self.momentum_w4 = self.beta1*self.momentum_w4 + self.lr*self.deri_w4\n",
    "            self.momentum_b3 = self.beta1*self.momentum_b3 + self.lr*self.deri_b3\n",
    "            self.momentum_w3 = self.beta1*self.momentum_w3 + self.lr*self.deri_w3\n",
    "            self.momentum_b2 = self.beta1*self.momentum_b2 + self.lr*self.deri_b2\n",
    "            self.momentum_w2 = self.beta1*self.momentum_w2 + self.lr*self.deri_w2\n",
    "            self.momentum_b1 = self.beta1*self.momentum_b1 + self.lr*self.deri_b1\n",
    "            self.momentum_w1 = self.beta1*self.momentum_w1 + self.lr*self.deri_w1\n",
    "\n",
    "            self.b4 -= self.momentum_b4\n",
    "            self.w4 -= self.momentum_w4\n",
    "            self.b3 -= self.momentum_b3\n",
    "            self.w3 -= self.momentum_w3\n",
    "            self.b2 -= self.momentum_b2\n",
    "            self.w2 -= self.momentum_w2\n",
    "            self.b1 -= self.momentum_b1\n",
    "            self.w1 -= self.momentum_w1\n",
    "            \n",
    "        elif self.optimizer == 'SGD':\n",
    "            self.b4 -= self.lr*self.deri_b4\n",
    "            self.w4 -= self.lr*self.deri_w4\n",
    "            self.b3 -= self.lr*self.deri_b3\n",
    "            self.w3 -= self.lr*self.deri_w3\n",
    "            self.b2 -= self.lr*self.deri_b2\n",
    "            self.w2 -= self.lr*self.deri_w2\n",
    "            self.b1 -= self.lr*self.deri_b1\n",
    "            self.w1 -= self.lr*self.deri_w1\n",
    "            \n",
    "        \n",
    "\n",
    "        self.deri_w1 = np.zeros([self.sizes[1], self.sizes[0]])\n",
    "        self.deri_w2 = np.zeros([self.sizes[2], self.sizes[1]])\n",
    "        self.deri_w3 = np.zeros([self.sizes[3], self.sizes[2]])\n",
    "        self.deri_w4 = np.zeros([self.sizes[4], self.sizes[3]])\n",
    "        self.deri_b1 = np.zeros(self.sizes[1])\n",
    "        self.deri_b2 = np.zeros(self.sizes[2])\n",
    "        self.deri_b3 = np.zeros(self.sizes[3])\n",
    "        self.deri_b4 = np.zeros(self.sizes[4])\n",
    "    ################training phase###########    \n",
    "    def train(self, trainImages, trainLabels):\n",
    "        size = str(self.batch_size)\n",
    "        accuracy = 0\n",
    "        err_sum = 0.0\n",
    "        avg_err = 0.0\n",
    "        revision = 0\n",
    "\n",
    "        batch_start_time = time.time()\n",
    "        for m in range (self.batch_size):\n",
    "            revision_output = np.zeros([self.sizes[4]])\n",
    "            revision_output[trainLabels[m]] = 1.0\n",
    "            self.forwardProp(trainImages[m].flatten())\n",
    "            self.backProp(trainImages[m].flatten(), revision_output)\n",
    "\n",
    "            if np.argmax(self.outputNeurons) == int(trainLabels[m]):\n",
    "                revision+=1\n",
    "\n",
    "            error = np.amax(np.absolute(self.cceoutputLayerErrors))\n",
    "            print(\"Error:\")\n",
    "            print(error)\n",
    "            err_sum += error\n",
    "            avg_err = err_sum / (m+1)\n",
    "            acc = str(int((revision/(m+1))*100)) + '%'\n",
    "            print(\"Accuracy:\")\n",
    "            print(acc)\n",
    "            \n",
    "            \n",
    "        self.Update_weight()\n",
    "        return avg_err\n",
    "    ################prediction###########\n",
    "    def predict(self, testImage):\n",
    "        self.forwardProp(testImage)\n",
    "        return np.argmax(self.outputNeurons), self.outputNeurons\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase based on sigmoid activation and SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images = train_images/255\n",
    "inst_mlp = MLP([784, 128,64,64, 10], 'SGD', 'sigmoid','sigmoid','sigmoid', 'softmax')\n",
    "train_err=[]\n",
    "test_err=[]\n",
    "for i in range (inst_mlp.epochs):\n",
    "    print('----------------------'+'Epoch'+str(i+1)+'----------------------')\n",
    "    for j in range(int(inst_mlp.samples/inst_mlp.batch_size)):\n",
    "        print(\"Epoch\", str(i+1) + \"/\" + str(inst_mlp.epochs) + \":\" )\n",
    "        inst_mlp.y[j+i*(int(inst_mlp.samples/inst_mlp.batch_size))] = inst_mlp.train(train_images[int(j * inst_mlp.batch_size):int(j * inst_mlp.batch_size) + inst_mlp.batch_size], train_labels[int(j * inst_mlp.batch_size):int(j * inst_mlp.batch_size) + inst_mlp.batch_size])\n",
    "    size1 = test_images.shape[0]\n",
    "    temp_err=0\n",
    "    for i in range (size1):\n",
    "        prediction,yp = inst_mlp.predict(test_images[i].flatten())\n",
    "        yt = np.zeros([10])\n",
    "        yt[test_labels[i]] = 1.0\n",
    "        cost=Multi_cross_entropy(yt,yp)\n",
    "        temp_err=temp_err+np.amax(np.absolute(cost))\n",
    "        \n",
    "    test_err.append(temp_err)\n",
    "test_err=np.array(test_err)/10000       \n",
    "for i in range(inst_mlp.epochs):\n",
    "    train_err.append((np.sum(inst_mlp.y[60000*i:60000*(i+1)-1]))/60000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfalse = 0\n",
    "size1 = train_images.shape[0]\n",
    "for i in range (size1):\n",
    "    prediction,t = inst_mlp.predict(train_images[i].flatten())\n",
    "    ytrue = int(train_labels[i])\n",
    "    if ytrue != prediction:\n",
    "        yfalse += 1  \n",
    "\n",
    "print('Train_Accuracy:')\n",
    "print(((size1-yfalse)/size1)*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfalse = 0\n",
    "size1 = test_images.shape[0]\n",
    "for i in range (size1):\n",
    "    prediction,t = inst_mlp.predict(test_images[i].flatten())\n",
    "    ytrue = int(test_labels[i])\n",
    "    if ytrue != prediction:\n",
    "        yfalse += 1  \n",
    "\n",
    "print('Train_Accuracy:')\n",
    "print(((size1-yfalse)/size1)*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot cost function for training phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,inst_mlp.epochs+1), np.array(train_err))\n",
    "plt.ylabel('Test Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot cost function for testing phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,inst_mlp.epochs+1), np.array(test_err))\n",
    "plt.ylabel('Train Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with PyTorch (784,128,64,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transform))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MLP with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_pytorch(nn.Module):\n",
    "    def __init__(self, epochs):\n",
    "        super(MLP_pytorch, self).__init__()\n",
    "        self.epochs = epochs\n",
    "        self.first_layer = nn.Linear(784, 128)\n",
    "        self.second_layer = nn.Linear(128, 64)\n",
    "        self.third_layer = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        x = self.first_layer(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.second_layer(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.third_layer(x)\n",
    "        x = torch.softmax(x, dim=0)\n",
    "        return x\n",
    "\n",
    "    def train(self, train_loader, test_loader, optimizer, criterion):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_err=[]\n",
    "        train_acc=[]\n",
    "        test_err=[]\n",
    "        test_acc=[]\n",
    "\n",
    "        for iteration in range(self.epochs):\n",
    "            loss = 0\n",
    "            ep_err=0\n",
    "            ep_err_test=0\n",
    "            correct=0\n",
    "            correct_test=0\n",
    "            for x,y in train_loader:\n",
    "                ytrue=y.item()\n",
    "                encoded = torch.zeros([10], dtype=torch.float64)\n",
    "                encoded[y[0]] = 1\n",
    "                y = encoded\n",
    "                optimizer.zero_grad()\n",
    "                output = self.forward_pass(torch.flatten(x))\n",
    "                ypred=torch.argmax(output).item()\n",
    "                loss = criterion(output, y)\n",
    "                ep_err=ep_err+loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if(ypred==ytrue):\n",
    "                    correct=correct+1\n",
    "            for x,y in test_loader:\n",
    "                ytrue=y.item()\n",
    "                encoded = torch.zeros([10], dtype=torch.float64)\n",
    "                encoded[y[0]] = 1\n",
    "                y = encoded\n",
    "                output = self.forward_pass(torch.flatten(x))\n",
    "                ypred=torch.argmax(output).item()\n",
    "                loss = criterion(output, y)\n",
    "                ep_err_test=ep_err_test+loss\n",
    "                if(ypred==ytrue):\n",
    "                    correct_test=correct_test+1\n",
    "                    \n",
    "                \n",
    "            \n",
    "                #print('Epoch: {0}, Time Spent: {1:.2f}s, Loss: {2}'.format(\n",
    "                #iteration+1, time.time() - start_time, loss))\n",
    "            train_acc.append(correct/60000)\n",
    "            train_err.append(ep_err.item()/60000)\n",
    "            test_acc.append(correct_test/60000)\n",
    "            test_err.append(ep_err_test.item()/60000)\n",
    "            print(\"Epoch \"+str(iteration)+\" done!\")\n",
    "            print(\"Train Error:\")\n",
    "            print(ep_err.item()/60000)\n",
    "            print(\"Test Error:\")\n",
    "            print(ep_err_test.item()/60000)\n",
    "            print(\"Train accuracy:\")\n",
    "            print(correct/60000)\n",
    "            print(\"Test accuracy:\")\n",
    "            print(correct_test/60000)\n",
    "            \n",
    "        return train_err, train_acc, test_err, test_acc\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_pytorch(3)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "tr_err, tr_acc, te_err, te_acc=model.train(train_loader,test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot test cost  function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,4), np.array(tr_err))\n",
    "plt.ylabel('Train Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot train cost  function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,4), np.array(te_err))\n",
    "plt.ylabel('Train Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
